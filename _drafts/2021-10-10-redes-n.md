---
layout: post
title: "Rendimentos decrescentes em NLP"
comments: true
mathjax: true
description: "A solução de código"
keywords: "LSTM, NLP, Deep Learning"
---

Em Economia, a lei dos [rendimentos descrescentes](https://en.wikipedia.org/wiki/Diminishing_returns) fala sobre a relação entre trabalho e produção, que tende a piorar à medida em que se investe mais trabalho. De forma mais precisa, dado: $$ Entrada = E $$, $$ Saida = S $$ e $$ S = f(E) $$. A teoria diz que: $$ 2 \times f(E) > f(2 \times E) $$.

Talvez não por esse nome, mas essa dinâmica é muito familiar às pessoas que lidam com modelagem. Aumentar o desempenho de um modelo vai se tornando mais complicado à medida em que se avança, como podemos ver nas soluções desenvolvidas em [cenários competitivos](https://analyticsindiamag.com/how-useful-was-the-netflix-prize-really/).

Em outras palavras: melhorar os últimos 5% de um modelo é muito mais difícil que os primeiros 10%. No contexto de problemas reais, é muito importante ter em mente essa dinâmica, pois há um momento em que não faz mais sentido continuar evoluindo um modelo.

Nesse post, quero explorar um pouco essa dinâmica em problemas de NLP. Quando vale a pena, pular de uma solução mais simples baseada em *bag of words*, para algo mais sofisticado envolvendo *embeddings* e redes neurais? Aliás, realmente vale a pena fazer esse pulo?

A proposta de experimento não é muito inovadora: usando um [corpus do Skoob](/2019/07/27/corpus-skoob.html), a ideia é comparar abordagens em diferentes níveis de complexidade para resolver um problema de classificação. Um problema comum e – justamente pela sua prevalência – interessante para fazer essa análise da relação "custo/benefício" de diferentes abordagens.

## Neural NLP 

A área de NLP passou por uma [revolução](https://jmlr.csail.mit.edu/papers/volume12/collobert11a/collobert11a.pdf), com o uso de *word embeddings* e redes neurais convolucionais/recorrentes. São conceitos relativamente independentes – e que já existiam há tempos – mas que algumas evoluções tornaram seu uso muito interessante para problemas de NLP.

Um dos maiores exemplos de sucesso dessa nova abordagem, foram os sistemas de tradução automática. Deixaram de ser sistemas complexos e altamente especializados, para modelos de linguagem condicionais, chamados [Seq2seq](https://en.wikipedia.org/wiki/Seq2seq). 

Um modelo Seq2seq propõe uma solução mais universal e simples para o problema de tradução. Quando os avanços de NLP dependiam de recursos sofisticados, como corpus [anotados por linguistas](https://en.wikipedia.org/wiki/Treebank) e [ontologias](https://wordnet.princeton.edu/frequently-asked-questions), era muito díficil adaptar soluções de outros idiomas para língua portuguesa.

Devido a essa dificuldade histórica, era complexo evoluir modelos em língua portuguesa para algo mais sofisticado. Nem sempre era possível adaptar uma solução de outro idioma que dependia de recursos linguísticos sofisticados (*e.g.* ontologias, árvores de dependência, gramáticas).

Nesse novo cenário, de modelos mais agnósticos, podemos seguir por dois caminhos:

* Soluções baseadas em *[bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)*, que a despeito das claras limitações, são muito simples de implementar e acabam atendendo vários casos de uso.

* Soluções baseadas em redes neurais, que lida melhor com muito das limitações dos modelos *bag of words* e são mais próximas do estado da arte, mas demandam mais esforço de desenvolvimento e poder computacional.

Há um degrau claro de complexidade entre as as duas abordagens, mas é dificil estimar o custo/benefício de ultrapassá-lo. Pensando em  elucidar um pouco essa questão, experimentei resolver um mesmo problema usando as duas abordagens.

## O problema ~~manjado~~ de resenhas

Anos atrás, eu fiz um script para extrair as [resenhas do Skoob](/2019/07/27/corpus-skoob.html), que não usei para nada até esse experimento. Felizmente ainda funciona, consegui gerar um dataset das resenhas feitas pelos usuários.

Sendo um corpus de resenhas, o leitor já deve suspeitar do plano: fazer um classificador de notas a partir do texto das resenhas, o clássico problema de análise de sentimentos[^1].

[^1]: não gosto do termo "análise de sentimentos", acho pretensioso demais e o conceito mal definido. Por exemplo, é válido chamar um classificador de estrelas como de sentimentos?

Ao escrever uma resenha, o usuário do Skoob tem a opção de atribuir estrelas ao livro. As estrelas variam no intervalo $$  [1,5] $$, mas optei por trabalhar apenas com o subconjunto $$ \{1, 3, 5\} $$ para testar os classificadores.

Optei por essa estratégia, porque essa gradação de notas subjetivas é uma questão difícil de se trabalhar, tanto que YouTube e Netflix migraram de um sistema de estrelas para um de notas binárias. O plano original era usar somente os valores extremos $$ \{1,5\} $$, entretanto as classes ficariam muito desbalanceadas dessa forma e dificultaria a análise. 

Há detalhes conceituais em lidar com gradações, que são díficeis de resolver: por exemplo, será que existe um critério comum entre pessoas para distinguir entre 4 e 5 estrelas? É uma discussão interessante e importante, mas apartada do objetivo original desse estudo.

Filtrando o corpus pelo critério de seleção, o dataset final ficou com um total **828.118** resenhas: **23.893** com 1 estrela,  **174.836** com 3 estrelas e **629.389** com 5 estrelas.

## Comparando as estratégias

Definido o problema de classificação, a ideia é comparar duas abordagens macro: uma baseada em redes neurais e *word-embeddings* e outra utilizando classificadores mais simples e representação *bag of words*.

Não será uma comparação completa entre as soluções, mas comparar as soluções mais imediatas dessa abordam. Ou seja, por onde normalmente se começa a resolver um problema de classificação de textos.

O problema de classificação é multi-classe e desbalanceado, aspectos que aumentam a complexidade na interpretação de resultados. Usarei acurácia simples como referência, mas no post colocarei a matriz de confusão para que facilitar a análise do leitor.

Para trabalhar com os mesmos dados, vou utilizar 90% do conjunto (**745.307**  amostras) para treino e os 10% restantes (**82.811** amostras) para validação. 

## Montando o baseline

Para começar um experimento de classificação em NLP, acredito que o caminho mais clássico possível é usar um classificador Naive Bayes. É um classificador simples, que separa as classes baseada na diferença das distribuições de palavras que compõe os textos de cada classe.

Uma limitação desse classificador – que está em seu próprio nome – é a premissa "ingênua" de que as dimensões são independentes entre si. Ou seja, que as ocorrências das palavras dentro de um texto são independentes entre si. Outra limitação, mas essa proveniente da representação *bag of words*, é desconsiderar a ordem das palavras.

A despeita dessas limitações, é um classificador [bem adaptado](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf) ao problema de classificação de texto. Além de ser muito simples de utilizar e rápido para ser treinado e fazer predições.

Abaixo, os resultados obtidos por esse modelo:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/MultinomialNB.svg"/>
  <figcaption>Figura 1 – Resultados Naive Bayes </figcaption>
</figure>

Os resultados foram obtidos usando a configuração padrão do [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB):

```python
clf = MultinomialNB()
```

 Testei a possibilidade de ignorar a probabilidade *a priori* (`fit_prior = False`), mas impactou negativamente o desempenho.

Para fazer a transformação dos textos, usei o [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do scikit-learn com uma lista de stopwords:

```python
vectorizer = CountVectorizer(stop_words=stopwords.words('portuguese'))
```

A lista de stopwords é proveniente do [NLTK](https://gist.github.com/alopes/5358189). Além das stopwords, experimentei aplicar o [RSLP](https://www.nltk.org/_modules/nltk/stem/rslp.html), um algoritmo de stemming para língua portuguesa. Nenhum desses pré-processamento tiveram muito impacto no classificador, seja positivo ou negativo.

Infelizmente, não consegui testar com [n-gram](https://en.wikipedia.org/wiki/N-gram) maior que 2, devido a falta de memória no computador. De qualquer forma, os melhores resultados foram obtidos sem o uso de n-grams no Naive Bayes.

## Subindo o baseline com SVM e TF-IDF

Um outro classificador, bastante utilizado em problemas de NLP, é o [SVM linear](https://link.springer.com/chapter/10.1007%2FBFb0026683). 

Textos representados como vetores *bag of words* possuem alta dimensão, o que aumenta a chance de serem linearmente separáveis, entretanto pode ser díficil ajustar um modelo com dados esparsos. O treinamento do SVM – que é feito via a otimização distância entre as amostras de diferentes classes – acaba sendo uma boa opção para esse tipo de problema.

Em conjunto com o SVM, é interessante usar [tf-idf](https://en.wikipedia.org/wiki/Tf–idf) para destacar as palavras mais importantes do corpus, ao invés de simples fazer a contagem das palavras como no caso do Naive Bayes. É uma representação muito usada em problemas de [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval), mas que também pode ser útil em classificação.


No SVM, é possível variar alguns parâmetros como formulação da otimização e penalização das margens, mas em testes rápidos vi poucas diferenças. Assim como o classificador *Naive Bayes*, optei por usar o SVM com os hiper-parâmetros padrão do scikit-learn:

```python
clf = LinearSVC()
```

Além de usar td-idf para prencher as dimensões, o uso de 2-grams para representação também ajudou a obter melhores resultados:

```python
vectorizer = TfidfVectorizer(ngram_range=(1,2))
```
Usando essa configuração, já é possível obter ganhos significativos de desempenho, especialmente nas classes negativa e neutra:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/LinearSVC.svg"/>
  <figcaption>Figura 2 – Resultados SVM </figcaption>
</figure>

A implementação desses modelos é muito simples, pode ser feita em um dia, além de serem simples de rodar em qualquer computador. Com esse baseline, podemos partir para a solução mais complexa com o uso de redes neurais e embeddings.

## Usando LSTM e word-embeddings

Ao trabalhar com soluções mais complexas, a primeira dificuldade são as muitas decisões a serem tomadas: arquitetura/tamanho da rede, hiper-parâmetros, embeddings treináveis ou não, algoritmo de treinamento, etc...

Devido a essa complexidade, acredito que muitos possam discordar que o proposta é a solução baseline ideal. Entrentanto, esse aspecto já faz parte da análise: como um leigo em redes neurais para NLP começaria? Os resultados são expressivamente melhores ou demandam mais habilidade/tempo?

Deixando essa discussão para o final, vamos a solução que desenvolvi.

### Arquitetura da rede

A rede neural utilizado foi uma LSTM bi-direcional, talvez a arquitetura mais popular de RNN para textos. Há soluções que usam somente a saída final para classificação, mas até mesmo por serem textos longos, acho melhor trabalhar com a saída de cada "etapa" da LSTM:

```python
model = tf.keras.Sequential([
    encoder,
    tf.keras.layers.Embedding(
        input_dim=len(encoder.get_vocabulary()),
        output_dim=EMBEDDING_DIM,
        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix) if USE_EMBEDDING else None,
        trainable=True,
        mask_zero=True),
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, return_sequences=True, stateful=False)),
    tf.keras.layers.Dense(SEQUENCE_LENGTH, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(.6),
    tf.keras.layers.Dense(3, activation='softmax')
])
```

O tamanho da rede ficou limitada pela memória da minha GPU (Nvidia GTX 1650), que possui apenas 4GB de memória. A versão final com esse "tamanho":

* Camada de embeddings com 300 dimensões.
* LSTM com 300 neurônios.
* Camada densa com 500 neurônios (tamanho máximo da sequência).
* Camada de dropout com .6 de probabilidade.
* Batch de 32 resenhas

Abaixo, o `summary` do modelo:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
text_vectorization (TextVect (None, 1000)              0         
_________________________________________________________________
embedding (Embedding)        (None, 1000, 300)         15467100  
_________________________________________________________________
bidirectional (Bidirectional (None, 1000, 600)         1442400   
_________________________________________________________________
dense (Dense)                (None, 1000, 1000)        601000    
_________________________________________________________________
flatten (Flatten)            (None, 1000000)           0         
_________________________________________________________________
dropout (Dropout)            (None, 1000000)           0         
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3000003   
=================================================================
Total params: 20,510,503
Trainable params: 20,510,503
Non-trainable params: 0
_________________________________________________________________
```

Não é possível aumentar muito esse modelo, ele já está no limite da memória disponível (4GB) da minha GPU.

### Embeddings pré-treinados

Uma das possibilidades mais interessantes das redes neurais, é usar embeddings pré-treinados. Para português, o grupo [NILC](http://www.nilc.icmc.usp.br/embeddings) tem um repositório com embeddings gerados por diferentes métodos: Word2Vec, FastText , Wang2Vec e Glove. 

Para esse experimento, testei o Wang2Vec e Glove. Na [avaliação do grupo](https://arxiv.org/pdf/1708.06025.pdf), o Glove apresentou melhor desempenho na avaliação intrínseca por analogias e o Wang2Vec na avaliação extrínseca (POS-tagging e similaridade de sentenças).

A diferença entre os métodos é mínima, como esperado, mas o Wang2Vec acabou se saindo ligeiramente melhor nesse experimento. No futuro, pretendo explorar mais essse tópico de embeddings e avaliar avanços como subword embeddings.

Para integrar esse embeddings no modelo, é relativamente simples. Usando o gensim é simples de fazer a leitura deles, indexado por palavra:

```python
from gensim.models import KeyedVectors

EMBEDDING_DIM = 300
USE_EMBEDDING = True
EMBEDDING_TYPE = 'skip'

base_dir = 'sa-experiments/corpus'

if USE_EMBEDDING:
    embeddings_index = KeyedVectors.load_word2vec_format(f'{base_dir}/embeddings/{EMBEDDING_TYPE}_s{EMBEDDING_DIM}.txt')
```

Após carregar os embeddings, é possível criar uma matriz que será utilizada pela camada de embeddings da rede neural:

```python
if USE_EMBEDDING:
    voc = encoder.get_vocabulary()
    word_index = dict(zip(voc, range(len(voc))))

    num_tokens = len(voc)
    embedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))

    hits = 0
    misses = 0

    for word, i in word_index.items():
        
        if embeddings_index.has_index_for(word):
            embedding_matrix[i] = embeddings_index[word]
            hits+=1
        else:
            misses+=1

    print(f"Hits: {hits}")
    print(f"Misses: {misses}")
```

### Pré-processamento do texto

Assim como nos métodos bag-of-words, não apliquei nenhum pré-processamento sofisticado no texto, simplesmente fiz a tokenização do texto.

Ao definir uma rede LSTM, é necessário definir um tamanho da sequência. Poderia usar o tamanho máximo do texto, mas seria um problema ao falar de resenhas sem limite de tamanho.

Gerei o histograma do tamanho das resenhas, é possível perceber que a maioria está na ordem de centenas de palavras, mas podem ser muito maiores também:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/contagem_palavras.svg"/>
  <figcaption>Figura 3 – Histograma do tamanho das resenhas </figcaption>
</figure>

Optei por deixar um limite de 1000 palavras, mais de 99% das resenhas tem 1000 ou menos palavras.

<!-- Comentar vocabulário -->