---
layout: post
title: "Rendimentos decrescentes em NLP"
comments: true
mathjax: true
description: "A solução de código"
keywords: "LSTM, NLP, Deep Learning"
---

Em Economia, a lei dos [rendimentos descrescentes](https://en.wikipedia.org/wiki/Diminishing_returns) fala sobre a relação entre trabalho e produção, que tende a piorar à medida em que se investe mais trabalho. De forma mais precisa, dado: $$ Entrada = E $$, $$ Saida = S $$ e $$ S = f(E) $$. A teoria diz que: $$ 2 \times f(E) > f(2 \times E) $$.

Talvez não por esse nome, mas essa dinâmica é muito familiar às pessoas que lidam com modelagem. Aumentar o desempenho de um modelo vai se tornando mais complicado à medida em que se avança, como podemos ver nas soluções desenvolvidas em [cenários competitivos](https://analyticsindiamag.com/how-useful-was-the-netflix-prize-really/).

Em outras palavras: melhorar os últimos 5% de um modelo é muito mais difícil que os primeiros 10%. No contexto de problemas reais, é muito importante ter em mente essa dinâmica, pois há um momento em que não faz mais sentido continuar evoluindo um modelo.

Nesse post, quero explorar um pouco essa dinâmica em problemas de NLP. Quando vale a pena, pular de uma solução mais simples baseada em *bag of words*, para algo mais sofisticado envolvendo *embeddings* e redes neurais? Aliás, realmente vale a pena fazer esse pulo?

A proposta não é muito inovadora: usando um [corpus do Skoob](/2019/07/27/corpus-skoob.html), a ideia é comparar abordagens em diferentes níveis de complexidade para resolver um problema de classificação. Um problema comum e – justamente pela sua prevalência – interessante para fazer essa análise da relação "custo/benefício" de diferentes abordagens.

## Neural NLP 

A área de NLP passou por uma [revolução](https://jmlr.csail.mit.edu/papers/volume12/collobert11a/collobert11a.pdf), com o uso de *word embeddings* e redes neurais convolucionais/recorrentes. São conceitos relativamente independentes – e que já existiam há tempos – mas que algumas evoluções tornaram seu uso muito interessante para problemas de NLP.

Um dos exemplos de sucesso dessa nova abordagem, são os sistemas de tradução automática. Deixaram de ser sistemas complexos e altamente especializados, para modelos de linguagem condicionais, chamados [Seq2seq](https://en.wikipedia.org/wiki/Seq2seq). 

Quando os avanços de NLP dependiam de recursos sofisticados, como corpus [anotados por linguistas](https://en.wikipedia.org/wiki/Treebank) e [ontologias](https://wordnet.princeton.edu/frequently-asked-questions), era muito díficil adaptar soluções de outros idiomas para língua portuguesa. Um modelo Seq2seq propõe uma solução mais universal e simples para o problema de tradução.

Nesse novo cenário, de modelos mais agnósticos, podemos seguir por dois caminhos:

* Soluções baseadas em *[bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)*, que a despeito das claras limitações, são muito simples de implementar e acabam atendendo vários casos de uso.

* Soluções baseadas em redes neurais, que lida melhor com muito das limitações dos modelos *bag of words* e são mais próximas do estado da arte, mas demandam mais esforço de desenvolvimento e poder computacional.

Há um degrau claro de complexidade entre as as duas abordagens, mas é dificil estimar o custo/benefício de ultrapassá-lo. Pensando em  elucidar um pouco essa questão, experimentei resolver um mesmo problema usando as duas abordagens.

## O problema ~~manjado~~ de resenhas

Anos atrás, eu fiz um Scrapy para extrair as [resenhas do Skoob](/2019/07/27/corpus-skoob.html), que não usei para nada até esse experimento. Felizmente o site não mudou nesse período, então foi possível re-extrair as resenhas com dados até Outubro de 2021.

Se tratando de um corpus de resenhas, o leitor já deve suspeitar do plano: fazer um classificador de notas, o clássico problema de análise de sentimentos[^1].

[^1]: não gosto do termo "análise de sentimentos", acho pretensioso demais e o conceito mal definido. Por exemplo, é válido chamar um classificador de estrelas como de sentimentos?

Ao escrever uma resenha, o usuário do Skoob tem a opção de atribuir estrelas ao livro. As estrelas variam no intervalo $$  [1,5] $$, mas optei por trabalhar apenas com o subconjunto $$ \{1, 3, 5\} $$ para testar os classificadores.

Optei por essa estratégia, porque essa gradação de notas subjetivas é uma questão difícil de se trabalhar, tanto que YouTube e Netflix migraram de um sistema de estrelas para um de notas binárias. O plano original era usar somente os valores extremos $$ \{1,5\} $$, entretanto as classes ficariam muito desbalanceadas dessa forma e dificultaria a análise dos resultados. 

Há detalhes conceituais em lidar com gradações, que são díficeis de resolver: por exemplo, será que existe um critério comum entre pessoas para distinguir entre 4 e 5 estrelas? É uma discussão interessante e importante, mas apartada do objetivo original desse estudo.

Filtrando o corpus pelo critério de seleção, o dataset final ficou com um total **828.118** resenhas: **23.893** com 1 estrela,  **174.836** com 3 estrelas e **629.389** com 5 estrelas.

## Comparando as estratégias

Definido o problema de classificação, a ideia é comparar duas abordagens macro: uma baseada em redes neurais e *word-embeddings* e outra utilizando classificadores mais simples e representação *bag of words*.

Não será uma comparação completa entre as soluções, mas comparar as soluções mais imediatas dessas abordagens. Ou seja, por onde normalmente se começa a resolver um problema de classificação de textos.

O problema é multi-classe e desbalanceado, aspectos que aumentam a complexidade na interpretação de resultados. Usarei acurácia simples como referência, mas no post colocarei a matriz de confusão para facilitar a análise do leitor.

Para comparar os resultados, vou utilizar 90% do conjunto (**745.307**  amostras) para treino e os 10% restantes (**82.811** amostras) para validação. 

## Montando o baseline

Para começar um experimento de classificação em NLP, acredito que o caminho mais clássico possível é usar um classificador Naive Bayes. É um classificador simples, que separa as classes baseada na diferença das distribuições de palavras que compõe os textos de cada classe.

Uma limitação desse classificador – que está em seu próprio nome – é a premissa "ingênua" de que as dimensões são independentes entre si. Ou seja, que as ocorrências das palavras dentro de um texto são independentes entre si. Outra limitação, mas essa proveniente da representação *bag of words*, é desconsiderar a ordem das palavras.

A despeita dessas limitações, é um classificador [bem adaptado](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf) ao problema de classificação de texto. Além de ser muito simples de utilizar e rápido para ser treinado e fazer predições.

Abaixo, os resultados obtidos por esse modelo:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/MultinomialNB.svg"/>
  <figcaption>Figura 1 – Resultados Naive Bayes </figcaption>
</figure>

Esses resultados foram obtidos usando a configuração padrão do [`MultinomialNB`](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB):

```python
clf = MultinomialNB()
```

 Testei a possibilidade de ignorar a probabilidade *a priori* (`fit_prior = False`), mas impactou negativamente o desempenho.

Para fazer a transformação dos textos, usei o [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do scikit-learn e uma lista de stopwords (proveniente do [NLTK](https://gist.github.com/alopes/5358189)) a serem ignoradas:

```python
vectorizer = CountVectorizer(stop_words=stopwords.words('portuguese'))
```

Além das stopwords, experimentei aplicar o [RSLP](https://www.nltk.org/_modules/nltk/stem/rslp.html), um algoritmo de stemming para língua portuguesa. É um pré-processamento pesado e que não teve muito impacto no desempenho do classificador.

Infelizmente, não consegui testar com [n-gram](https://en.wikipedia.org/wiki/N-gram) maior que 2, devido a falta de memória no computador. De qualquer forma, os melhores resultados foram obtidos sem o uso de n-grams no Naive Bayes.

## Subindo o baseline com SVM e TF-IDF

Um outro classificador, bastante utilizado em problemas de NLP, é o [SVM linear](https://link.springer.com/chapter/10.1007%2FBFb0026683). 

Textos representados como vetores *bag of words* são esparsos e possuem alta dimensão, o que aumenta a chance de serem linearmente separáveis. Por outro lado, pode ser complexo ajustar um modelo com vetores esparsos.

O treinamento do SVM – que é feito via a otimização distância entre as amostras de diferentes classes – acaba sendo uma boa opção para esse tipo de problema.

No SVM, é possível variar alguns parâmetros como formulação da otimização e penalização das margens, mas em testes rápidos vi poucas diferenças. Assim como o classificador *Naive Bayes*, optei por usar o SVM com os hiper-parâmetros padrão do scikit-learn:

```python
clf = LinearSVC()
```

Uma possibilidade que interessante do SVM, é utilizá-lo com a representação [tf-idf](https://en.wikipedia.org/wiki/Tf–idf), para destacar as palavras mais importantes do corpus, ao invés de simples fazer a contagem das palavras. É uma representação muito usada em problemas de [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval), mas que também pode ser útil em classificação.

Além de usar td-idf para prencher as dimensões, nesse caso o uso de 2-grams ajudou a obter melhores resultados:

```python
vectorizer = TfidfVectorizer(ngram_range=(1,2))
```
Usando essa configuração, já é possível obter ganhos significativos de desempenho, especialmente nas classes negativa e neutra:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/LinearSVC.svg"/>
  <figcaption>Figura 2 – Resultados SVM </figcaption>
</figure>

A implementação desse modelo é muito simples, pode ser feita em um dia, além de ser simples de rodar em qualquer computador. Com esse baseline, podemos partir para a solução mais complexa com o uso de redes neurais e embeddings.

## Usando LSTM e word-embeddings

Ao trabalhar com soluções mais complexas, a primeira dificuldade são as muitas decisões a serem tomadas: arquitetura/tamanho da rede, hiper-parâmetros, embeddings treináveis ou não, algoritmo de treinamento, etc...

Devido a essa complexidade, acredito que muitos possam discordar que minha proposta é a solução baseline ideal. Entrentanto, esse aspecto já faz parte da questão em pauta: como um leigo em redes neurais para NLP começaria? Bons resultados demandam demandam mais habilidade/tempo para serem alcançados?

Deixando essa discussão para o final, vamos a solução testada.

### Arquitetura da rede

A rede neural utilizado foi uma LSTM bi-direcional, talvez a arquitetura mais popular de RNN para textos. Há soluções que usam somente a saída final para classificação, mas até mesmo por serem textos longos, acho melhor trabalhar com a saída de cada "etapa" da LSTM:

```python
emb = tf.keras.layers.Embedding(mask_zero=True, 
                                input_dim=len(encoder.get_vocabulary()),
                                output_dim=EMBEDDING_DIM, 
                                trainable=True)

model = tf.keras.Sequential([
    encoder,
    emb,
    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(300, return_sequences=True, stateful=False)),
    tf.keras.layers.Dense(SEQUENCE_LENGTH, activation='relu'),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dropout(.6),
    tf.keras.layers.Dense(3, activation='softmax')
])

emb.set_weights([embedding_matrix])
```

O tamanho da rede ficou limitada pela memória da minha GPU, que possui apenas 4GB de memória. A versão final com esse "tamanho":

* Camada de embeddings com 300 dimensões.
* LSTM com 300 neurônios.
* Camada densa com 1.000 neurônios.
* Camada de dropout com .6 de probabilidade.
* Batch de 16 resenhas

Abaixo, o `summary` do modelo:

```
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
text_vectorization (TextVect (None, 1000)              0         
_________________________________________________________________
embedding (Embedding)        (None, 1000, 300)         15467100  
_________________________________________________________________
bidirectional (Bidirectional (None, 1000, 600)         1442400   
_________________________________________________________________
dense (Dense)                (None, 1000, 1000)        601000    
_________________________________________________________________
flatten (Flatten)            (None, 1000000)           0         
_________________________________________________________________
dropout (Dropout)            (None, 1000000)           0         
_________________________________________________________________
dense_1 (Dense)              (None, 3)                 3000003   
=================================================================
Total params: 20,510,503
Trainable params: 20,510,503
Non-trainable params: 0
_________________________________________________________________
```

### Embeddings pré-treinados

Uma das possibilidades mais interessantes das redes neurais, é usar embeddings pré-treinados. Para português, o grupo [NILC](http://www.nilc.icmc.usp.br/embeddings) tem um repositório com embeddings gerados por diferentes métodos: Word2Vec, FastText , Wang2Vec e Glove. Para esse experimento, testei o Wang2Vec e Glove. 

Na [avaliação do grupo](https://arxiv.org/pdf/1708.06025.pdf), o Glove apresentou melhor desempenho na avaliação intrínseca por analogias e o Wang2Vec na avaliação extrínseca (POS-tagging e similaridade de sentenças). A diferença entre os métodos é mínima, como esperado, mas o Wang2Vec acabou se saindo ligeiramente melhor nesse experimento.

Para integrar esse embeddings no modelo, é relativamente simples. Usando o gensim é simples de fazer a leitura deles, indexado por palavra:

```python
from gensim.models import KeyedVectors

EMBEDDING_DIM = 300
USE_EMBEDDING = True
EMBEDDING_TYPE = 'skip'

base_dir = 'sa-experiments/corpus'

if USE_EMBEDDING:
    embeddings_index = KeyedVectors.load_word2vec_format(f'{base_dir}/embeddings/{EMBEDDING_TYPE}_s{EMBEDDING_DIM}.txt')
```

Após carregar os embeddings, é possível criar uma matriz que será utilizada pela camada de embeddings da rede neural:

```python
if USE_EMBEDDING:
    voc = encoder.get_vocabulary()
    word_index = dict(zip(voc, range(len(voc))))

    num_tokens = len(voc)
    embedding_matrix = np.zeros((num_tokens, EMBEDDING_DIM))

    hits = 0
    misses = 0

    for word, i in word_index.items():
        
        if embeddings_index.has_index_for(word):
            embedding_matrix[i] = embeddings_index[word]
            hits+=1
        else:
            misses+=1

    print(f"Hits: {hits}")
    print(f"Misses: {misses}")
```

No futuro, pretendo explorar mais essse tópico de embeddings e avaliar avanços mais recentes como subword embeddings.

Uma limitação importante é o tamanho do vocabulário, já que não foi possível treinar a rede com ele completo. Foram usadas "apenas" a 200.000 palavras mais comuns no corpus, sendo um total de 136.725 presentes nos embeddings. 

O dataset completo tem mais de 600.000 palavras no vocabulário, é interessante fazer uma investigação posterior do impacto dessa restrição no classificador.

### Pré-processamento do texto

Assim como nos métodos bag-of-words, não apliquei nenhum pré-processamento sofisticado para a rede neural.

Para usar a LSTM, é necessário definir o tamanho máximo da sequência [^2]. Poderia usar o tamanho máximo de amostra no corpus, mas seria um problema já que o tamanho das resenhas varia muito. 

Pelo histograma do tamanho das resenhas, é possível perceber que a maioria está na ordem de centenas de palavras, mas há uma cauda longa:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/contagem_palavras.svg"/>
  <figcaption>Figura 3 – Histograma do tamanho das resenhas </figcaption>
</figure>

Por esses números, optei por deixar um limite de 1000 palavras, 99% das resenhas têm 1000 ou menos palavras. Poucas resenhas serão cortadas por esse limite, devendo impactar pouco no desempenho do modelo.

### Treinamento

O treinamento da rede neural foi feita usando Adam e entropia cruzada como função de perda:

```python
model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
              optimizer=tf.keras.optimizers.Adam(1e-4),
              metrics=[tf.keras.metrics.CategoricalAccuracy()])
```

O treinamento foi feito utilizando a seguinte configuração, com o driver 495.44 da Nvidia:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/neofetch.png"/>
  <figcaption>Figura 3 – Configuração </figcaption>
</figure>

Nesse cenário, cada época demorou cerca de 2 horas, abaixo o relatório de 5 épocas executadas para ajustar o modelo:


### Resultados

Usando essa solução de rede neural, foi obtido um resultado similar de acurácia geral: 0,86 para o SVM e 0,87 para a LSTM.

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/LSTM.svg"/>
  <figcaption>Figura 4 – Resultados LSTM </figcaption>
</figure>

O SVM teve melhor desempenho na classe negativa, com 0,81 de acurácia contra 0,78 da LSTM. Por outro lado, a LSTM teve 0,77 de acurácia para a classe neutra enquanto o SVM chegou em 0,72. 

## Conclusão

Olhando esses resultados, podemos dizer que os resultados são praticamente o mesmo entre as duas soluções. Não há muito o que ponderar sobre a relação custo/benefício para esse problema, a solução bag-of-words é muito mais simples e chegou a resultados similares.

A complexidade da solução é subjetiva, alguém mais experiente em redes neurais poderia ter desenvolvido essa solução em um fração do tempo que demorei. Entretanto, os limites de hardware e tempo de treinamento são um problema, especialmente no cenário atual de GPUs.

Há muito que explorar na solução de redes neurais, incluindo aspectos óbvios como aumentar a rede e o vocabulário, mas no limite do meu hardware e competência as diferenças de resultado são mínimas. Um próximo passo óbvio é experimentar esse mesmo modelo sem as limitações de hardware.

Por fim, essa proposta de rede neural não é o estado de arte, há muita coisa acontecendo que pretendo estudar e explorar e posso explorar nesse dataset.

No final, há mais questões em aberto que respondidas com esse experimento, mas a não ser que eu tenha feito algo muito errado...parece que não é tão simples dar um pulo de desempenho saindo de bag of words para redes neurais.


[^2]: há opção de fazer uma LSTM stateful, mas implicaria em outra arquitetura de rede sem a camada densa.

