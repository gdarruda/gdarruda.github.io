---
layout: post
title: "Rendimentos decrescentes em NLP"
comments: true
mathjax: true
description: "A solução de código"
keywords: "LSTM, NLP, Deep Learning"
---

Em Economia, a lei dos [rendimentos descrescentes](https://en.wikipedia.org/wiki/Diminishing_returns) fala sobre a relação entre trabalho e produção, que tende a piorar à medida em que se investe mais trabalho. De forma mais precisa, dado: $$ Entrada = E $$, $$ Saida = S $$ e $$ S = f(E) $$. A teoria diz que: $$ 2 \times f(E) > f(2 \times E) $$. 

Talvez não por esse nome, mas essa dinâmica de rendimentos decrescentes, é muito familiar às pessoas que lidam com modelagem. Aumentar o desempenho de um modelo, se torna mais complicado à medida em que se avança, como podemos ver nas soluções desenvolvidas para cenários competitivos como o [desafio Netflix](https://analyticsindiamag.com/how-useful-was-the-netflix-prize-really/).

Em resumo: melhorar os últimos 5% de um modelo é muito mais difícil que os primeiros 10%. Em um contexto de negócios, é muito importante ter em mente essa dinâmica, pois há um momento em que não faz mais sentido investir na modelagem.

Pensando em problemas de NLP: quando vale a pena pular de uma solução mais simples, baseada em *bag of words* por exemplo, para algo mais sofisticado envolvendo *embeddings* e redes neurais? Aliás, realmente vale a pena fazer esse pulo?

Pensando nessa questão, vou explorar um pouco esse problema trabalhando com o clássico problema de classificação de resenhas, usando o [corpus do Skoob]((/2019/07/27/corpus-skoob.html)) que discuti em um post anterior.

## Neural NLP 

A área de NLP passou por uma [revolução](https://jmlr.csail.mit.edu/papers/volume12/collobert11a/collobert11a.pdf), com o uso de *word embeddings* e redes neurais convolucionais/recorrentes. São conceitos relativamente independentes – e que já existiam há tempos – mas que algumas evoluções tornaram seu uso muito interessante para problemas de NLP.

Um dos maiores exemplos de sucesso dessa nova abordagem, foram os sistemas de tradução automática. Deixaram de ser sistemas complexos e altamente especializados, para modelos de linguagem condicionais, chamados [Seq2seq](https://en.wikipedia.org/wiki/Seq2seq). 

Um modelo Seq2seq propõe uma solução mais universal e simples para o problema de tradução. Quando os avanços de NLP dependiam de recursos sofisticados, como corpus [anotados por linguistas](https://en.wikipedia.org/wiki/Treebank) e [ontologias](https://wordnet.princeton.edu/frequently-asked-questions), era muito díficil adaptar soluções de outros idiomas para língua portuguesa.

No contexto antigo, em que é complexo adaptar ferramentas para um contexto multi-linguagem, são comuns soluções baseadas em contagem de palavras. Evita-se usar soluções com conceitos mais complexos, que dependam de recursos pouco disponíveis (*e.g.* ontologias, árvores de dependência, gramáticas).

Nesse novo cenário – de modelos poderosos mais genéricos – abre-se um novo caminho. Para problemas de NLP em português, podemos seguir por dois caminhos:

* Soluções baseadas em *[bag of words](https://en.wikipedia.org/wiki/Bag-of-words_model)*, que a despeito das claras limitações, são muito simples de implementar e acabam atendendo vários casos de uso.

* Soluções baseadas em redes neurais, que lida melhor com muito das limitações dos modelos *bag of words* e são mais próximas do estado da arte, mas demandam mais esforço de desenvolvimento e poder computacional.

Há um degrau claro de complexidade entre as as duas abordagens, mas é dificil estimar o custo/benefício de subi-lo. Para elucidar um pouco essa questão, minha proposta é abordar um problema de classificação a partir das duas abordagens.

## O problema ~~manjado~~ de resenhas

Anos atrás, eu fiz um script para extrair [resenhas do Skoob](/2019/07/27/corpus-skoob.html) feitas pelos usuários, que não usei para nada até esse momento. 

Sendo um corpus de resenhas, o leitor já deve suspeitar do plano: fazer um classificador de notas a partir do texto das resenhas, o clássico problema de análise de sentimentos[^1].

[^1]: não gosto do termo "análise de sentimentos", acho pretensioso demais e o conceito mal definido. Por exemplo, é válido chamar um classificador de estrelas como de sentimentos?

Além de escrever a resenha, o usuário do Skoob tem a opção de atribuir estrelas ao livro. Como de costume, as estrelas variam no intervalo $$  [1,5] $$, mas optei por trabalhar apenas com o subconjunto $$ \{1, 3, 5\} $$ para fazer o classificador.

Optei por essa estratégia, porque essa gradação de notas subjetivas é uma questão difícil de se trabalhar, tanto que YouTube e Netflix migraram de um sistema de estrelas para um de notas binárias. Inclusive, o meu plano usar apenas o valores extremos $$ \{1,5\} $$, entretanto as classes ficariam muito desbalanceadas. 

Há muitos questionamentos possíveis para esse meu recorte do *corpus*, optei pela redução de classes para simplificar a definição do problema. Há detalhes conceituais em lidar com gradações, que merecem uma análise apartada. Por exemplo, será que existe um critério comum entre pessoas para distinguir entre 4 e 5 estrelas?

Como a ideia é mais comparar abordagens de modelagem e não discutir um problema real, acredito que esse recorte não seja uma questão crucial. Filtrando o corpus pelo critério de seleção, o dataset final ficou com um total **828.118** resenhas: **23.893** com 1 estrela,  **174.836** com 3 estrelas e **629.389** com 5 estrelas.

## Comparando as estratégias

Definido o problema de classificação, a ideia é comparar duas abordagens, uma baseada em redes neurais e *word-embeddings* e outra utilizando classificadores mais simples e representação *bag of words*.

Não será uma comparação completa entre as soluções, pois não haverá o ajuste fino dos classificadores e preparação de dados. Uma análise mais exaustiva das abordagens seria muito bom, mas entendo que essa comparação inicial já é um material interessante.

Sendo um problema multi-classe e desbalanceado, acho bom trabalhar com matriz de confusão, ao invés focar em uma métrica de comparação. Dessa forma, o leitor pode avaliar melhor os resultados.

Sendo um conjunto de dados grande, vou separá-lo em validação com 90% (**745.307**  amostras) para treino e os 10% restantes (**82.811** amostras) para validação. 

## Montando o baseline

Para começar um experimento de classificação em NLP, acredito que o caminho mais clássico possível é usar um classificador Naive Bayes. É um classificador simples, que separa as classes baseada na diferença das distribuições de palavras que compõe os textos de cada classe.

Uma limitação desse classificador – que está em seu próprio nome – é a premissa "ingênua" de que as dimensões são independentes entre si. Ou seja, que as ocorrências das palavras dentro de um texto são independentes entre si. Outra limitação, mas essa proveniente da representação *bag of words*, é desconsiderar a ordem das palavras.

A despeita dessas limitações, é um classificador simples e [bem adaptado](https://www.cs.unb.ca/~hzhang/publications/FLAIRS04ZhangH.pdf) a problemas que lidam com textos. Abaixo, os primeiros resultados do experimento:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/MultinomialNB.svg"/>
  <figcaption>Figura 1 – Resultados Naive Bayes </figcaption>
</figure>

Esses resultados foram obtidos usando o classificador na forma mais básica possível. Para fazer a transformação dos textos, usei o [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) do scikit-learn:

```python
vectorizer = CountVectorizer(stop_words=stopwords.words('portuguese'))
```

A lista de stopwords é proveniente do [NLTK](https://gist.github.com/alopes/5358189). Além das stopwords, experimentei usar o [RSLP](https://www.nltk.org/_modules/nltk/stem/rslp.html), um algoritmo de stemming para língua portuguesa. Nenhum desses pré-processamento tiveram muito impacto no classificador. 

Infelizmente, não consegui usar [n-gram](https://en.wikipedia.org/wiki/N-gram) maior que 2, devido a falta de memória no computador. De qualquer forma, os melhores resultados foram obtidos sem o uso de n-grams.

Em sua versão multinomial, não há muitos hiper-parâmetros no classificador [Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html). Há a possibilidade de ignorar a probabilidade *a priori*, mas desconsiderá-la impactou negativamente o desempenho.

## Subindo o baseline com SVM e TF-IDF

Um outro classificador, bastante utilizado em problemas de NLP, é o [SVM linear](https://link.springer.com/chapter/10.1007%2FBFb0026683). 

A representação *bag of words* geram vetores de alta dimensão e esparsos. A alta dimensão desses vetores facilitam com que sejam separáveis linearmente, entretanto pode ser díficil ajustar um modelo com dados esparsos. O treinamento do SVM – que é feito via a otimização distância entre as amostras de diferentes classes – sendo ideal para esse tipo de problema.

Uma outra vantagem desse modelo, é ser possível usar [tf-idf](https://en.wikipedia.org/wiki/Tf–idf) para destacar as palavras mais importantes do corpus ao representar um texto. É uma representação muito usada em problemas de [information retrieval](https://en.wikipedia.org/wiki/Information_retrieval), mas que também pode ser útil em classificação.

O melhor resultado foi usando a representação com o uso de 2-grams, além das palavras:

```python
vectorizer = CountVectorizer(ngram_range=(1,2))
```

Em relação classificador, é possível variar alguns parâmetros como formulação da otimização e penalização das margens, mas em testes rápidos vi poucas diferenças. Assim como o classificador *Naive Bayes*, optei por usar o SVM com os hiper-parâmetros padrão do scikit-learn.

A partir dessa combinação, já chegamos a resultados bem melhores, especialmente nas classes negativa e neutra:

<figure>
  <img src="{{site.url}}/assets/images/ml-skoob/LinearSVC.svg"/>
  <figcaption>Figura 2 – Resultados SVM </figcaption>
</figure>

Usando uma LSTM, será que esse desempenho sobe de patamar?

## Partindo para LSTM e word-embeddings

Ao trabalhar com redes neurais, a primeira dificuldade são as decisões a serem tomadas: arquitetura/tamanho da rede, hiper-parâmetros, embeddings treinados ou não, algoritmo de treinamento, etc...

